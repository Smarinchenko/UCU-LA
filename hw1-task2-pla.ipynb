{"cells":[{"cell_type":"code","execution_count":1,"id":"fe44083c-b2d1-4370-a08c-53b19b8ec944","metadata":{"id":"fe44083c-b2d1-4370-a08c-53b19b8ec944"},"outputs":[],"source":["# We use torch and sklearn only on steps of fitting data\n","import torch\n","from tqdm import tqdm\n","from scipy.optimize import linprog\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from sklearn.model_selection import train_test_split\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"]},{"cell_type":"markdown","id":"EQznMLwjRzhi","metadata":{"id":"EQznMLwjRzhi"},"source":["# Problem 2: Separating hyperplanes and the Perceptron Learning Algorithm (3pts)\n","### <div align=\"right\"> &copy; Yurii Yeliseev & Rostyslav Hryniv, 2022 </div>\n","\n","## Completed by:   \n","*   First team member\n","*   Second team member"]},{"cell_type":"markdown","id":"euyTTMP6nrDO","metadata":{"id":"euyTTMP6nrDO"},"source":["### Description:\n","#### The aim of this task is to discuss a simple binary classification method for linearly separated classes. The Perceptron Learning Algorithm finds a ***separating hyperplane*** in finitely many steps and is based on a clear geometric update method. We will derive the upper bound on the number of iterations in PLA and implement it for digit classification for the MNIST database.\n","\n","#### For this task of your homework you can get 3 points. Do not forget to save and rename the notebook before making any changes!"]},{"cell_type":"markdown","id":"BPiRUcHnhqxX","metadata":{"id":"BPiRUcHnhqxX"},"source":["## 1. Separating hyperplanes and classification (0.9 pts)"]},{"cell_type":"markdown","id":"F8uu18FS0NVJ","metadata":{"id":"F8uu18FS0NVJ"},"source":["### 1.1. Binary classification.    \n","A typical task of binary classification reads as follows. We are given the set of labelled (training) data $(\\mathbf{x}_k, y_k), k=1,2,\\dots, N$, where $\\mathbf{x}_k \\in \\mathbb{R}^d$ gives a data point and the label $y_k = \\pm1$ encodes the class (e.g. $y_k=1$ is the <font color='red'>''red''</font> class and $y_k=-1$ is the <font color='blue'>''blue''</font> one). The task is to find a classfier $f \\,:\\, \\mathbb{R}^d \\to \\pm1$ that would correctly recognize the classes, i.e. satisfy $y_k f(\\mathbf{x}_k) >0$ for all (or most) $k=1,2,\\dots,N$. This function can then be used to guess the class of new (unseen) data $\\mathbf{x}\\in\\mathbb{R}^n$.\n"]},{"cell_type":"markdown","id":"HqpK65XMSYul","metadata":{"id":"HqpK65XMSYul"},"source":["\n","### 1.2. Separating hyperplane  \n","The simplest case is when the red and blue classes are *linearly separable*, i.e., when there is a hyperplane $H: \\mathbf{w} \\cdot \\mathbf{x} + w_0 = 0$ separating the red and blue datapoints. Then  $f(\\mathbf{x}) = \\mathbf{w}\\cdot \\mathbf{x} + w_0$ is an affine classifier, so that $f(\\mathbf{x}_k)>0$ for red points and $f(\\mathbf{x}_k)<0$ for blue ones. Augmenting $\\mathbf{x}$ to $\\widehat{\\mathbf{x}} := (1, \\mathbf{x})$ and $\\widehat{\\mathbf{w}} = (w_0,\\mathbf{w})$, we recognize that $f(\\mathbf{x})= \\widehat{\\mathbf{x}}\\cdot \\widehat{\\mathbf{w}}$. Therefore, the angles between $\\widehat{\\mathbf{x}}$ and $\\widehat{\\mathbf{w}}$ are acute for red datapoints and obtuse for the blue ones. The task is therefore to find the *normal vector* $\\widehat{\\mathbf{w}}$ with this properties."]},{"cell_type":"markdown","id":"75612201-411f-48d9-8f36-629d8e8c5811","metadata":{"id":"75612201-411f-48d9-8f36-629d8e8c5811","tags":[]},"source":["### 1.3. The idea behind the Perceptron learning algorithm (PLA)\n","\n","To simplify the notations, in what follows we will omit the \"hats\" above the $(d+1)$-dimensional vectors $\\widehat{\\mathbf{x}}$ and $\\widehat{\\mathbf{w}}$.\n","\n","PLA is an iterative algorithm that updates the direction vector ${\\mathbf{w}}$ towards a misclassified example, one at a time.\n","\n","Let's recall that correctly classified vectors $\\mathbf{x}_j$ must satisfy the inequality\n","$$\n","  y_j ({\\mathbf{w}}\\cdot {\\mathbf{x}}_j) > 0.\n","$$\n","If a red $\\mathbf{x}_j$ is misclassified, then the angle between ${\\mathbf{w}}$ and ${\\mathbf{x}}_j$ is obtuse. The idea is that we should decrease the angle between them by updating ${\\mathbf{w}}$ to ${\\mathbf{w}} + {\\mathbf{x}}_j$ (see Figure 1). Likewise, if a blue $\\mathbf{x}_j$ is misclassified, then the angle between ${\\mathbf{w}}$ and ${\\mathbf{x}}_j$ is acute, and we increase it be replacing ${\\mathbf{w}}$ with ${\\mathbf{w}} - {\\mathbf{x}}_j$. In both cases, the update is $${\\mathbf{w}} \\mapsto {\\mathbf{w}} + y_j {\\mathbf{x}}_j$$\n","\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n","    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","    <title></title>\n","</head>\n","<body>\n","    <img src=\"https://drive.google.com/uc?export=view&id=12rduejeedS8NxrxXkSBJkkcDH3lB0k-R\">\n","\n","\n","</body>\n","</html>\n","\n","### 1.4. **PLA**\n","\n","The above considerations suggest the following **PLA**:\n","1.   Start with ${\\mathbf{w}}_0=\\mathbf{0}$ and classify the points\n","2.   Take an arbitrary misclassified point\n","3.   Update the ${\\mathbf{w}}$\n","4.   Update the classification\n","5.   Repeat 2-4 until there are misclassified points.\n"]},{"cell_type":"markdown","id":"UBUdk2akoCMC","metadata":{"id":"UBUdk2akoCMC"},"source":["### 1.5. **PLA**: proof of convergence (0.9 pts)\n","\n","---"]},{"cell_type":"markdown","id":"oOn-vs9ye4B2","metadata":{"id":"oOn-vs9ye4B2"},"source":["#### **1.5.1 (0.3 pts)** Analyze the PLA update step  \n","Prove that by updating ${\\mathbf{w}}$, we are decreasing or increasing (as required) the angle between ${\\mathbf{w}}$ and ${\\mathbf{x}}_j$.\n","\n","---\n","\n","\\#### Your proof comes here \\####\n","\n","---\n","$cos(\\theta) = \\displaystyle\\frac{w\\cdot x}{||w||\\cdot||x||}.\\;$ Updating $w_{k+1} = w_k+x_t\\cdot y_t$ yields $cos(\\theta_{k+1}) = \\displaystyle\\frac{x_t\\cdot w_{k+1}}{||x_t||\\cdot ||w_{k+1}||} = \\displaystyle\\frac{x_t\\cdot w_{k} + y_t||x_t||^2}{||x_t||\\cdot ||w_{k} + y_t\\cdot x_t||}.\\;$ Since $x_t \\cdot w_k$ is always of the opposite sign to $y_t||x_t||$, $x_t\\cdot w_k + y_t||x_t||^2$ will increae negative cos($\\theta$) (and decrease obtuse angle $\\theta$) when red point is misclassified. And an update will decrease the positive $cos(\\theta)$ (and increase acute angle $\\theta$) when red point is misclassified."]},{"cell_type":"markdown","id":"e5680473-6119-4831-8990-ff6b3ac69e83","metadata":{"id":"e5680473-6119-4831-8990-ff6b3ac69e83"},"source":["#### **Assumptions and notations**\n","\n","***Assumption on linear separability*** There exists an ${\\mathbf{w}^{\\star}} \\in \\mathbb{R}^{d+1}$ of unit length and $\\gamma > 0$ such that $$y_k\\, {\\mathbf{x}}_k\\cdot {\\mathbf{w}}^{\\star} \\ge \\gamma, \\qquad k=1,2,\\dots, n.$$ The value $\\gamma$ determines the width of the *separating slab* free of any datapoints. The larger $\\gamma$, the wider the slab and the more robust the classifier is to noise in data.  \n","\n","We also denote by $R$ the maximum norm of $\\mathbf{x}_k$\n","\n","***Theorem on PLA convergence.*** The PLA makes at most $\\frac{R^2}{\\gamma^2}$ updates, after which it returns a separating hyperplane.\n","\n","***Proof.*** Should the algorthm terminate, then the resulting ${\\mathbf{w}}$ determines a separating hyperplane. Thus it suffices to show that the algorithm terminates after at most $\\frac{R^2}{\\gamma^2}$ updates. The approach is to get upper and lower bounds on the norm of the $k^{\\mathrm{th}}$ update ${\\mathbf{w}}_k$ of the weighting vector ${\\mathbf{w}}$, starting with ${\\mathbf{w}}_0 = \\mathbf{0}$.\n","\n","Assume that $k\\ge 1$ and ${\\mathbf{x}}_j$ is a misclasssified point on iteration $k$; then\n","$$\n","\\begin{aligned}\n","\\mathbf{w}_{k+1} \\cdot {\\mathbf{w}}^{\\star} &=\\left({\\mathbf{w}}_k + y_j \\mathbf{x}_j\\right) \\cdot {\\mathbf{w}}^{\\star} \\\\\n","&={\\mathbf{w}}_k \\cdot {\\mathbf{w}}^{\\star}+y_j\\left({\\mathbf{x}}_j \\cdot {\\mathbf{w}}^{\\star}\\right) \\\\\n","&>{\\mathbf{w}}_k \\cdot {\\mathbf{w}}^{\\star} + \\gamma\n","\\end{aligned}\n","$$\n","\n","---\n"]},{"cell_type":"markdown","id":"Quwk-bRAoONU","metadata":{"id":"Quwk-bRAoONU"},"source":["#### **1.5.2. (0.3 pts)** Explain by induction that ${\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star}> k \\gamma$.\n","\n","---\n","\n","\\#### Your proof comes here \\####\n","\n","---"]},{"cell_type":"markdown","id":"7d6d98a5","metadata":{},"source":["Induction step is already shown to hold in last paragraph. Applying the induction step recursively to inequality $w_{k+1}\\cdot w^{\\star} > w_k\\cdot w^{\\star} + \\gamma$ yields:\n","$\\\\ w_k \\cdot w^{\\star} > w_{k-1}\\cdot w^{\\star} + \\gamma \\;$ and $\\;w_{k-1}\\cdot w^{\\star} > w_{k-2}\\cdot w^{\\star} + \\gamma  \\implies w_k \\cdot w^{\\star} > w_{k-2}\\cdot w^{\\star} + 2\\gamma.\\;$ After k iterations we have: $w_k \\cdot w^{\\star} > w_{1}\\cdot w^{\\star} + k\\gamma.$"]},{"cell_type":"markdown","id":"3uu2Kmluod9X","metadata":{"id":"3uu2Kmluod9X"},"source":["As a result, we see that\n","$$\\|\\mathbf{w}_k\\| \\ge {\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star}> k \\gamma\\tag{1}$$\n","\n","To obtain the upper bound, we argue that\n","$$\n","\\begin{aligned}\n","\\left\\|\\mathbf{w}_{k+1}\\right\\|^2 &=\\left\\|\\mathbf{w}_k+y_j \\mathbf{x}_j\\right\\|^2 \\\\\n","&=\\left\\|\\mathbf{w}_k\\right\\|^2+\\left\\|y_j \\mathbf{x}_j\\right\\|^2+2\\left(\\mathbf{w}_k \\cdot \\mathbf{x}_j\\right) y_j \\\\\n","&=\\left\\|\\mathbf{w}_k\\right\\|^2+\\left\\|\\mathbf{x}_j\\right\\|^2+2\\left(\\mathbf{w}_k \\cdot \\mathbf{x}_j\\right) y_j\n","\\end{aligned}\n","$$\n","\n","---"]},{"cell_type":"markdown","id":"BfKkTkx2YTrb","metadata":{"id":"BfKkTkx2YTrb"},"source":["#### **1.5.3. (0.3 pts)** Derive the lower bound\n","$$\n","\\begin{aligned}\n","\\left\\|\\mathbf{w}_{k+1}\\right\\|^2\n","&\\le\\left\\|\\mathbf{w}_k\\right\\|^2+\\left\\|\\mathbf{x}_j\\right\\|^2 \\\\\n","&\\le\\left\\|\\mathbf{w}_k\\right\\|^2+R^2\n","\\end{aligned}\n","$$\n","and use induction to conclude that\n","$$\n","\\left\\|\\mathbf{w}_{k}\\right\\|^2 \\le k\\, R^2 \\tag{2}\n","$$\n","\n","---\n","\n","\\#### Your proof comes here \\####\n","\n","---\n"]},{"cell_type":"markdown","id":"a7bc95a6","metadata":{},"source":["The same reasoning applies. $||w_1|| \\leq ||w_0|| + R^2\\; and \\; ||w_2|| \\leq ||w_1|| + R^2\\ \\implies ||w_2|| \\leq ||w_0|| + 2R^2.$\n","\n","Repeating the step k times yields: $||w_k|| \\leq ||w_0|| + kR^2$. And since $||w_0|| = 0$, $||w_k|| \\leq kR^2.$"]},{"cell_type":"markdown","id":"eb0a9416","metadata":{},"source":[]},{"cell_type":"markdown","id":"Uk_bKxExoqXD","metadata":{"id":"Uk_bKxExoqXD"},"source":["\n","Together, (1) and (2) yield\n","$$\n","k^2 \\gamma^2<\\left\\|\\mathbf{w}_{k}\\right\\|^2 \\le k R^2,\n","$$\n","which implies the bound $k<\\frac{R^2}{\\gamma^2}$ and finishes the proof."]},{"cell_type":"markdown","id":"4vq40CCjb4tp","metadata":{"id":"4vq40CCjb4tp"},"source":["## 2. PLA implementation on MNIST dataset (1.8 pts)"]},{"cell_type":"markdown","id":"69405653-4bb7-4a5a-a6ab-6d06f81e2452","metadata":{"id":"69405653-4bb7-4a5a-a6ab-6d06f81e2452"},"source":["### 2.1. Data"]},{"cell_type":"markdown","id":"25024833-b7ae-48a4-b2d2-254aedb9e1ff","metadata":{"id":"25024833-b7ae-48a4-b2d2-254aedb9e1ff"},"source":["`train_data` is torch dataset object where images and targets lie inside `train_data.data` and `train_data.targets` respectively. To convert to numpy array you can use `.numpy()` method."]},{"cell_type":"code","execution_count":4,"id":"689dbfa9-1ed2-4baa-b0f0-8c094fa9a972","metadata":{"id":"689dbfa9-1ed2-4baa-b0f0-8c094fa9a972"},"outputs":[{"data":{"text/plain":["tensor([5, 0, 4,  ..., 5, 6, 8])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_data = datasets.MNIST(root='data', train=True, download=True)\n","train_data.targets"]},{"cell_type":"markdown","id":"ab0a6dd4-0ffe-4f59-bfad-b33824243772","metadata":{"id":"ab0a6dd4-0ffe-4f59-bfad-b33824243772"},"source":["### 2.2 Take 2 digits samples **(0.3 pts)**\n","\n","First of all you need to take only two digits samples from the dataset and convert the targets properly for the PLA. Choose the two digits based on the sum of your birthdays (e.g. 2 and 4 if it is 24; take 4 and 5 if it is 44)"]},{"cell_type":"code","execution_count":37,"id":"8cf9e98a-daff-4c94-9019-ace288e89bc9","metadata":{"id":"8cf9e98a-daff-4c94-9019-ace288e89bc9"},"outputs":[],"source":["def filter_data(train_data, digit_1, digit_2):\n","    \"\"\"\n","    Take only digit_1 and digit_2 from the dataset and transform labels\n","    Args:\n","        train_data: torchvision.datasets.mnist.MNIST\n","        digit_1: int (from 0 to 9)\n","        digit_2: int (from 0 to 9)\n","\n","    Returns:\n","        train_data: torchvision.datasets.mnist.MNIST or np.array\n","    \"\"\"\n","    # ========= YOUR CODE STARTS HERE ========= #\n","    digits_cls = torch.tensor([digit_1, digit_2])\n","    indices = torch.isin(train_data.targets, digits_cls)\n","    train_data.data, train_data.targets = train_data.data[indices], train_data.targets[indices]\n","    # ========== YOUR CODE ENDS HERE ========== #\n","    return train_data"]},{"cell_type":"code","execution_count":36,"id":"dee2eaa6-498f-4174-875a-501548d56113","metadata":{"id":"dee2eaa6-498f-4174-875a-501548d56113"},"outputs":[],"source":["train_data = filter_data(train_data, 3, 2)"]},{"cell_type":"markdown","id":"IFga5wKx5qLn","metadata":{"id":"IFga5wKx5qLn"},"source":["### 2.3 Take a smaller subset and divide it into train and test sets **(0.3 pts)**\n","\n","\n","Since the dataset is big, you need to use only part of it in this task (take\n","~20-30% of the whole dataset for further processing).\n","\n","1. Properly subdivide dataset\n","2. Calculate number samples in each class for test and train\n","\n","***Note***: you need to have same distributions inside train and test set"]},{"cell_type":"code","execution_count":46,"id":"13cxSAQd5qLo","metadata":{"id":"13cxSAQd5qLo"},"outputs":[],"source":["def split_dataset(train_data):\n","    \"\"\"\n","    Split dataset into train and test parts.\n","\n","    !Hint: You can use train_test_split from sklearn for that\n","\n","    Args:\n","        train_data: torchvision.datasets.mnist.MNIST or np.array\n","\n","    Returns:\n","        X_train: Array of shape (N, 28, 28), images from the train set\n","        y_train: Array of shape (N), labels from the train set\n","\n","        X_test: Array of shape (N, 28, 28), images from the test set\n","        y_test: Array of shape (N), labels from the test set\n","    \"\"\"\n","    # ========= YOUR CODE STARTS HERE ========= #\n","    X_train, X_test, y_train, y_test = train_test_split(train_data.data, train_data.targets, test_size=0.33, stratify = train_data.targets, random_state=42)\n","    X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.33, stratify = y_test, random_state=42)\n","    # ========== YOUR CODE ENDS HERE ========== #\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":47,"id":"4pT-osLT5qLo","metadata":{"id":"4pT-osLT5qLo"},"outputs":[],"source":["X_train, X_test, y_train, y_test = split_dataset(train_data)"]},{"cell_type":"code","execution_count":48,"id":"KV4wznVX5qLo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KV4wznVX5qLo","outputId":"2ec93ca1-af38-414f-fc25-349053e2c1b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of samples in train: 2673 \n","Classes number in train: 1356, 1317 \n","Number of samples in test: 1317 \n","Classes number in test: 668, 649\n"]}],"source":["print(f\"Number of samples in train: {len(X_train)} \\n\\\n","Classes number in train: {torch.sum(y_train == 3)}, {torch.sum(y_train == 2)} \\n\\\n","Number of samples in test: {len(X_test)} \\n\\\n","Classes number in test: {torch.sum(y_test == 3)}, {torch.sum(y_test == 2)}\")"]},{"cell_type":"markdown","id":"swE73GJ76GxU","metadata":{"id":"swE73GJ76GxU"},"source":["### 2.4 Visualize samples for the train set"]},{"cell_type":"code","execution_count":49,"id":"6jc6fJqX6Gxe","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"id":"6jc6fJqX6Gxe","outputId":"ebe6e978-ea59-41fe-bc61-2c52f565ef08"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1500x500 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["myd = {3:1, 2:-1}\n","y_train = np.vectorize(myd.get)(y_train)\n","\n","fig = plt.figure(figsize=(15, 5))\n","for idx in np.arange(10):\n","    ax = fig.add_subplot(2, 5, idx+1, xticks=[], yticks=[])\n","    ax.imshow(np.squeeze(X_train[idx].numpy()), cmap='gray')\n","    ax.set_title(str(y_train[idx].item()))"]},{"cell_type":"markdown","id":"X2el3UpD6cYO","metadata":{"id":"X2el3UpD6cYO"},"source":["### 2.5 Preprocess the samples and initialize $\\mathbf{w}$ **(0.4 pts)**"]},{"cell_type":"markdown","id":"8531c05a-28a8-4309-98d2-b278e43be21f","metadata":{"id":"8531c05a-28a8-4309-98d2-b278e43be21f"},"source":["The original algorithm starts from zero parameter vector, but actually we can use just randomly initialized vector; it will make it faster to converge\n","\n","**Instructions**: Complete the missing lines of code and calculate the performance on test set"]},{"cell_type":"code","execution_count":51,"id":"2a71993e-91fe-45af-bb58-040bbfd0cc40","metadata":{"id":"2a71993e-91fe-45af-bb58-040bbfd0cc40"},"outputs":[],"source":["def prep_data(X_train):\n","    \"\"\"\n","    Flatten, normalize and extra column for bias\n","    Args:\n","        X_train: np.array of shape (N, 28, 28)\n","\n","    Returns:\n","        X: preprocessed data\n","    \"\"\"\n","    # ========= YOUR CODE STARTS HERE ========= #\n","    \n","    X = X_train.reshape(X_train.shape[0], -1)\n","    vmean = np.apply_along_axis(np.mean, 1, X).reshape(-1, 1)\n","    vsd = np.apply_along_axis(np.std, 1, X).reshape(-1, 1)\n","    X = (X - vmean) / vsd\n","    X = np.append(X, np.ones((X.shape[0], 1)), 1)\n","    # ========== YOUR CODE ENDS HERE ========== #\n","    return X\n","\n","def initialize_weight_vector(size):\n","    \"\"\"\n","    Create random parameter vector\n","    Args:\n","        size: Number of elements\n","\n","    Returns:\n","        W: np.array of shape (size)\n","    \"\"\"\n","    # ========= YOUR CODE STARTS HERE ========= #\n","\n","    W = np.random.randn(size)\n","\n","    return W\n","    # ========== YOUR CODE ENDS HERE ========== #\n","\n","def misclassified(X, y, W):\n","    \"\"\"\n","    Calculate indices of missclasified points\n","    Args:\n","        X: np.array, training images\n","        y: np.array, training labels\n","        w: np.array, parameter vector\n","\n","    Returns:\n","        M: np.array of shape (m) - indices of missclasified points, where m is a number of missclasified points\n","    \"\"\"\n","    # ========= YOUR CODE STARTS HERE ========= #\n","\n","    # myd = {3:1, 2:-1}\n","    # y = np.vectorize(myd.get)(y)\n","\n","    cond = np.logical_or(np.logical_and(((X @ W) > 0).flatten(), (y < 0)), np.logical_and(((X @ W) < 0).flatten(), (y > 0)))\n","\n","    M = np.array(list(range(0, X.shape[0])))[cond]\n","    return M\n","    # ========== YOUR CODE ENDS HERE ========== #\n","\n","\n","X_train_flat_aug = prep_data(X_train)\n","W = initialize_weight_vector(X_train_flat_aug.shape[1])"]},{"cell_type":"markdown","id":"4c942c8e-1a2b-47f0-af9a-8ac09905c8d4","metadata":{"id":"4c942c8e-1a2b-47f0-af9a-8ac09905c8d4"},"source":["### 2.6 Training loop **(0.5 pts)**\n","Here you need to complete the training loop of the PLA algorithm. Observe that recalculation the misclassified set (Step 3 of the PLA algorithm) is the most costly (as we need to iterate through the whole train set). To speed up the algorithm convergence, we will do the following:\n","-  determine the set $S$ of misclassified datapoints\n","-  for every $\\mathbf{w}\\in S$ that is still misclassified, update the vector $\\mathbf{w}$\n","-  only after that recalculate the set $S$"]},{"cell_type":"code","execution_count":52,"id":"ac792433-7ba8-483a-929f-c10fe0353b86","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ac792433-7ba8-483a-929f-c10fe0353b86","outputId":"e0e5afe8-845a-4fb1-844e-f456798916a5","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["1741\n","212\n","705\n","429\n","994\n","773\n","1154\n","741\n","1058\n","696\n","809\n","789\n","465\n","924\n","467\n","527\n","665\n","891\n","551\n","828\n","636\n","731\n","437\n","614\n","474\n","553\n","421\n","945\n","675\n","777\n","360\n","485\n","558\n","439\n","483\n","648\n","657\n","523\n","502\n","659\n","426\n","496\n","346\n","491\n","493\n","497\n","325\n","573\n","359\n","481\n","318\n","592\n","480\n","467\n","396\n","490\n","390\n","411\n","431\n","437\n","324\n","583\n","362\n","622\n","453\n","538\n","326\n","253\n","308\n","446\n","330\n","467\n","276\n","572\n","396\n","313\n","319\n","521\n","300\n","322\n","287\n","446\n","301\n","421\n","248\n","383\n","281\n","428\n","336\n","274\n","325\n","324\n","315\n","300\n","254\n","361\n","308\n","439\n","357\n","327\n","396\n","319\n","228\n","367\n","326\n","374\n","228\n","381\n","327\n","253\n","263\n","380\n","331\n","376\n","352\n","324\n","420\n","498\n","275\n","232\n","347\n","362\n","229\n","272\n","314\n","418\n","273\n","288\n","255\n","386\n","317\n","382\n","288\n","493\n","190\n","382\n","270\n","315\n","280\n","403\n","273\n","248\n","236\n","327\n","280\n","472\n","346\n","298\n","410\n","366\n","218\n","328\n","204\n","326\n","266\n","253\n","318\n","328\n","306\n","435\n","210\n","419\n","283\n","449\n","283\n","318\n","321\n","258\n","200\n","376\n","279\n","258\n","244\n","264\n","184\n","404\n","283\n","250\n","356\n","236\n","177\n","260\n","251\n","297\n","194\n","381\n","308\n","280\n","194\n","238\n","268\n","337\n","162\n","350\n","252\n","222\n","262\n","367\n","169\n","336\n","214\n","275\n","202\n","252\n","287\n","325\n","193\n","411\n","229\n","386\n","155\n","242\n","172\n","233\n","214\n","242\n","252\n","267\n","247\n","263\n","258\n","299\n","231\n","301\n","183\n","260\n","178\n","215\n","162\n","238\n","166\n","354\n","220\n","199\n","227\n","311\n","163\n","193\n","150\n","199\n","194\n","200\n","215\n","325\n","213\n","153\n","233\n","293\n","234\n","289\n","217\n","237\n","262\n","239\n","193\n","246\n","175\n","210\n","155\n","213\n","157\n","279\n","139\n","180\n","199\n","192\n","189\n","265\n","186\n","281\n","177\n","270\n","200\n","287\n","177\n","270\n","136\n","228\n","101\n","176\n","150\n","256\n","263\n","178\n","206\n","207\n","184\n","181\n","120\n","160\n","111\n","184\n","164\n","195\n","135\n","228\n","194\n","191\n","131\n","176\n","185\n","191\n","139\n","196\n","167\n","283\n","140\n","101\n","138\n","163\n","161\n","134\n","163\n","231\n","232\n","175\n","163\n","167\n","159\n","230\n","164\n","249\n","201\n","148\n","133\n","262\n","150\n","212\n","166\n","128\n","135\n","145\n","185\n","128\n","96\n","200\n","142\n","160\n","137\n","179\n","178\n","223\n","163\n","214\n","179\n","163\n","93\n","113\n","102\n","165\n","135\n","171\n","192\n","154\n","145\n","124\n","154\n","139\n","174\n","218\n","110\n","284\n","139\n","107\n","94\n","200\n","132\n","169\n","136\n","137\n","137\n","132\n","140\n","229\n","143\n","151\n","124\n","187\n","101\n","175\n","186\n","99\n","117\n","249\n","117\n","156\n","125\n","116\n","152\n","139\n","88\n","158\n","181\n","182\n","169\n","154\n","171\n","164\n","99\n","142\n","143\n","108\n","82\n","159\n","134\n","160\n","128\n","149\n","123\n","237\n","106\n","143\n","119\n","189\n","93\n","242\n","109\n","115\n","92\n","137\n","73\n","184\n","115\n","249\n","88\n","143\n","113\n","222\n","100\n","126\n","73\n","144\n","96\n","146\n","142\n","180\n","105\n","116\n","115\n","179\n","92\n","95\n","71\n","205\n","117\n","104\n","105\n","102\n","107\n","130\n","131\n","135\n","117\n","175\n","76\n","138\n","65\n","117\n","149\n","168\n","131\n","90\n","94\n","94\n","92\n","123\n","81\n","102\n","53\n","132\n","67\n","152\n","54\n","98\n","62\n","138\n","91\n","179\n","82\n","142\n","101\n","110\n","60\n","188\n","128\n","127\n","53\n","101\n","86\n","161\n","68\n","106\n","53\n","169\n","74\n","127\n","112\n","99\n","78\n","135\n","72\n","94\n","117\n","114\n","81\n","151\n","45\n","122\n","82\n","75\n","113\n","144\n","101\n","66\n","70\n","93\n","70\n","105\n","58\n","143\n","93\n","74\n","92\n","107\n","94\n","131\n","46\n","117\n","110\n","97\n","54\n","147\n","86\n","57\n","89\n","115\n","99\n","103\n","70\n","159\n","72\n","74\n","77\n","86\n","58\n","141\n","83\n","147\n","150\n","134\n","45\n","89\n","89\n","109\n","106\n","101\n","61\n","65\n","81\n","133\n","92\n","135\n","71\n","51\n","71\n","79\n","65\n","71\n","39\n","70\n","62\n","73\n","77\n","107\n","71\n","129\n","88\n","146\n","75\n","108\n","54\n","94\n","92\n","111\n","71\n","108\n","57\n","95\n","58\n","136\n","41\n","128\n","54\n","103\n","46\n","107\n","44\n","99\n","61\n","60\n","36\n","63\n","74\n","89\n","93\n","101\n","57\n","130\n","98\n","59\n","86\n","59\n","55\n","119\n","57\n","94\n","49\n","107\n","32\n","110\n","64\n","123\n","67\n","92\n","49\n","105\n","54\n","86\n","38\n","57\n","46\n","98\n","43\n","58\n","81\n","106\n","74\n","125\n","72\n","52\n","86\n","71\n","51\n","87\n","94\n","39\n","34\n","67\n","17\n","64\n","59\n","124\n","53\n","88\n","62\n","48\n","67\n","70\n","68\n","61\n","71\n","127\n","85\n","122\n","30\n","40\n","38\n","88\n","31\n","116\n","75\n","75\n","30\n","101\n","81\n","106\n","95\n","88\n","41\n","119\n","85\n","97\n","50\n","74\n","49\n","100\n","71\n","96\n","29\n","63\n","74\n","47\n","71\n","64\n","51\n","120\n","57\n","57\n","34\n","49\n","50\n","132\n","54\n","50\n","74\n","52\n","27\n","66\n","50\n","41\n","35\n","129\n","55\n","65\n","47\n","83\n","38\n","53\n","53\n","71\n","37\n","32\n","39\n","86\n","43\n","87\n","45\n","80\n","44\n","59\n","62\n","33\n","17\n","96\n","37\n","32\n","34\n","56\n","32\n","56\n","44\n","156\n","36\n","44\n","17\n","51\n","22\n","54\n","36\n","36\n","32\n","29\n","52\n","35\n","37\n","32\n","25\n","27\n","41\n","38\n","27\n","43\n","30\n","54\n","29\n","86\n","23\n","56\n","68\n","56\n","22\n","49\n","53\n","60\n","64\n","123\n","60\n","63\n","18\n","25\n","44\n","18\n","35\n","92\n","10\n","46\n","47\n","35\n","28\n","62\n","13\n","72\n","18\n","55\n","44\n","83\n","33\n","77\n","18\n","47\n","24\n","121\n","19\n","48\n","59\n","48\n","10\n","63\n","17\n","66\n","34\n","43\n","21\n","81\n","9\n","29\n","6\n","30\n","25\n","61\n","48\n","33\n","40\n","34\n","36\n","38\n","34\n","32\n","39\n","38\n","31\n","36\n","89\n","66\n","8\n","15\n","7\n","53\n","18\n","52\n","39\n","19\n","45\n","16\n","38\n","20\n","11\n","11\n","42\n","32\n","23\n","46\n","23\n","46\n","14\n","21\n","8\n","71\n","13\n","28\n","25\n","33\n","40\n","41\n","17\n","12\n","49\n","21\n","5\n","32\n","27\n","37\n","7\n","52\n","4\n","11\n","14\n","14\n","38\n","58\n","13\n","23\n","6\n","32\n","32\n","21\n","2\n","18\n","6\n","21\n","32\n","40\n","26\n","43\n","14\n","54\n","16\n","2\n","10\n","3\n","15\n","54\n","40\n","8\n","17\n","3\n","6\n","29\n","29\n","87\n","45\n","10\n","10\n","15\n","9\n","3\n","16\n","35\n","16\n","5\n","22\n","31\n","14\n","54\n","7\n","28\n","22\n","40\n","10\n","60\n","4\n","3\n","15\n","39\n","8\n","44\n","9\n","28\n","14\n","40\n","4\n","8\n","38\n","2\n","27\n","29\n","2\n","37\n","17\n","1\n","12\n","0\n"]}],"source":["\"\"\"\n","    Returns:\n","        W: the final vector of weights for the separating hyperplane\n","\"\"\"\n","\n","misclass = np.array([2])\n","\n","while (misclass.shape[0] != 0):\n","\n","    misclass = misclassified(X_train_flat_aug, y_train, W)\n","    print(len(misclass))\n","\n","    for j in range(0, misclass.shape[0]):\n","\n","        current_x, current_y = X_train_flat_aug[misclass[j], :], y_train[misclass[j]]\n","\n","        if (current_y * (current_x @ W) < 0):\n","            W += current_y * current_x"]},{"cell_type":"markdown","id":"151d7f57-bdec-4b35-9bca-22778c7778e9","metadata":{"id":"151d7f57-bdec-4b35-9bca-22778c7778e9"},"source":["### 2.7 Evaluate performance of the linear classifier on the test set **(0.3 pts)**\n","\n","Check your classifier on the test set. Think of possible metrics that characterize performance and comment on how good the classifier is"]},{"cell_type":"code","execution_count":62,"id":"ea11a6c7-e4a3-439d-a836-19455f3d643a","metadata":{"id":"ea11a6c7-e4a3-439d-a836-19455f3d643a"},"outputs":[{"name":"stdout","output_type":"stream","text":["The accuracy of the classifier on the test set equls 0.9476082004555809\n"]}],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","X_test_flat_aug = prep_data(X_test)\n","\n","X_test_flat_aug.shape\n","\n","pred = np.sign(X_test_flat_aug @ W)\n","\n","y_test01 = np.vectorize(myd.get)(y_test)\n","acc = (pred == y_test01).mean()\n","\n","print(f\"The accuracy of the classifier on the test set equls {acc}\")\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","id":"xq-iaKD2jgYU","metadata":{"id":"xq-iaKD2jgYU"},"source":["## 3. Conclusions **(0.3 pts)**\n","\n","Summarize in a few sentences what you have learned and achieved by completing the tasks of this assignment\n","\n","\n","\\### **YOUR ANSWER HERE** \\###"]},{"cell_type":"markdown","id":"8e854436","metadata":{},"source":["We have learned to implement the perceptron algorithm from scratch using simple linear algebra operations. The algorithm uses the geometry of sample space and constructs the optimal separating hyperplane that splits all samples into two classes."]}],"metadata":{"colab":{"collapsed_sections":["UBUdk2akoCMC","69405653-4bb7-4a5a-a6ab-6d06f81e2452","ab0a6dd4-0ffe-4f59-bfad-b33824243772","IFga5wKx5qLn","swE73GJ76GxU","X2el3UpD6cYO","4c942c8e-1a2b-47f0-af9a-8ac09905c8d4","151d7f57-bdec-4b35-9bca-22778c7778e9"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
